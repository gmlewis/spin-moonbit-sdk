// Generated by `wit-bindgen` 0.41.0. DO NOT EDIT!

///| Inference request parameters
pub(all) struct InferencingParams {
  max_tokens : UInt
  repeat_penalty : Float
  repeat_penalty_last_n_token_count : UInt
  temperature : Float
  top_k : UInt
  top_p : Float
} derive(Show, Eq)

///| The set of errors which may be raised by functions in this interface
pub(all) enum Error_ {
  ModelNotSupported
  RuntimeError(String)
  InvalidInput(String)
} derive(Show, Eq)

///| Usage information related to the inferencing result
pub(all) struct InferencingUsage {
  prompt_token_count : UInt
  generated_token_count : UInt
} derive(Show, Eq)

///| An inferencing result
pub(all) struct InferencingResult {
  text : String
  usage : InferencingUsage
} derive(Show, Eq)

///| Usage related to an embeddings generation request
pub(all) struct EmbeddingsUsage {
  prompt_token_count : UInt
} derive(Show, Eq)

///| Result of generating embeddings
pub(all) struct EmbeddingsResult {
  embeddings : Array[FixedArray[Float]]
  usage : EmbeddingsUsage
} derive(Show, Eq)

///| Perform inferencing using the provided model and prompt with the given optional params
pub fn infer(
  model : String,
  prompt : String,
  params : InferencingParams?,
) -> Result[InferencingResult, Error_] {
  let (lowered, lowered3, lowered4, lowered5, lowered6, lowered7, lowered8) = match
    params {
    None => (0, 0, (0.0 : Float), 0, (0.0 : Float), 0, (0.0 : Float))
    Some(payload2) =>
      (
        1,
        payload2.max_tokens.reinterpret_as_int(),
        payload2.repeat_penalty,
        payload2.repeat_penalty_last_n_token_count.reinterpret_as_int(),
        payload2.temperature,
        payload2.top_k.reinterpret_as_int(),
        payload2.top_p,
      )
  }
  let return_area = @ffi.malloc(20)
  wasmImportInfer(
    @ffi.str2ptr(model),
    model.length(),
    @ffi.str2ptr(prompt),
    prompt.length(),
    lowered,
    lowered3,
    lowered4,
    lowered5,
    lowered6,
    lowered7,
    lowered8,
    return_area,
  )
  let lifted21 = match @ffi.load8_u(return_area + 0) {
    0 => {
      let result = @ffi.ptr2str(
        @ffi.load32(return_area + 4),
        @ffi.load32(return_area + 8),
      )
      Result::Ok(InferencingResult::{
        text: result,
        usage: InferencingUsage::{
          prompt_token_count: @ffi.load32(return_area + 12).reinterpret_as_uint(),
          generated_token_count: @ffi.load32(return_area + 16).reinterpret_as_uint(),
        },
      })
    }
    1 => {
      let lifted = match @ffi.load8_u(return_area + 4) {
        0 => Error_::ModelNotSupported
        1 => {
          let result17 = @ffi.ptr2str(
            @ffi.load32(return_area + 8),
            @ffi.load32(return_area + 12),
          )
          Error_::RuntimeError(result17)
        }
        2 => {
          let result20 = @ffi.ptr2str(
            @ffi.load32(return_area + 8),
            @ffi.load32(return_area + 12),
          )
          Error_::InvalidInput(result20)
        }
        _ => panic()
      }
      Result::Err(lifted)
    }
    _ => panic()
  }
  ignore(model)
  ignore(prompt)
  @ffi.free(return_area)
  return lifted21
}

///| Generate embeddings for the supplied list of text
pub fn generate_embeddings(
  model : String,
  text : Array[String],
) -> Result[EmbeddingsResult, Error_] {
  let cleanupList : Array[@ffi.Cleanup] = []
  let ignoreList : Array[&@ffi.Any] = []
  let address = @ffi.malloc(text.length() * 8)
  for index = 0; index < text.length(); index = index + 1 {
    let element : String = text[index]
    let base = address + index * 8
    @ffi.store32(base + 4, element.length())
    @ffi.store32(base + 0, @ffi.str2ptr(element))
    ignoreList.push(element)
  }
  let return_area = @ffi.malloc(16)
  wasmImportGenerateEmbeddings(
    @ffi.str2ptr(model),
    model.length(),
    address,
    text.length(),
    return_area,
  )
  let lifted15 = match @ffi.load8_u(return_area + 0) {
    0 => {
      let array : Array[FixedArray[Float]] = []
      for index4 = 0; index4 < @ffi.load32(return_area + 8); index4 = index4 + 1 {
        let base3 = @ffi.load32(return_area + 4) + index4 * 8
        let result = @ffi.ptr2float_array(
          @ffi.load32(base3 + 0),
          @ffi.load32(base3 + 4),
        )
        array.push(result)
      }
      @ffi.free(@ffi.load32(return_area + 4))
      Result::Ok(EmbeddingsResult::{
        embeddings: array,
        usage: EmbeddingsUsage::{
          prompt_token_count: @ffi.load32(return_area + 12).reinterpret_as_uint(),
        },
      })
    }
    1 => {
      let lifted = match @ffi.load8_u(return_area + 4) {
        0 => Error_::ModelNotSupported
        1 => {
          let result11 = @ffi.ptr2str(
            @ffi.load32(return_area + 8),
            @ffi.load32(return_area + 12),
          )
          Error_::RuntimeError(result11)
        }
        2 => {
          let result14 = @ffi.ptr2str(
            @ffi.load32(return_area + 8),
            @ffi.load32(return_area + 12),
          )
          Error_::InvalidInput(result14)
        }
        _ => panic()
      }
      Result::Err(lifted)
    }
    _ => panic()
  }
  ignore(model)
  @ffi.free(address)
  @ffi.free(return_area)
  cleanupList.each(fn(cleanup) { @ffi.free(cleanup.address) })
  ignore(ignoreList)
  return lifted15
}
