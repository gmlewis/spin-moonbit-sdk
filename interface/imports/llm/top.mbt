// Generated by `wit-bindgen` 0.29.0. DO NOT EDIT!

///| Inference request parameters
pub(all) struct InferencingParams {
  max_tokens : UInt
  repeat_penalty : Double
  repeat_penalty_last_n_token_count : UInt
  temperature : Double
  top_k : UInt
  top_p : Double
} derive(Show, Eq)

///| The set of errors which may be raised by functions in this interface
pub(all) enum Error_ {
  ModelNotSupported
  RuntimeError(String)
  InvalidInput(String)
} derive(Show, Eq)

///| Usage information related to the inferencing result
pub(all) struct InferencingUsage {
  prompt_token_count : UInt
  generated_token_count : UInt
} derive(Show, Eq)

///| An inferencing result
pub(all) struct InferencingResult {
  text : String
  usage : InferencingUsage
} derive(Show, Eq)

///| Usage related to an embeddings generation request
pub(all) struct EmbeddingsUsage {
  prompt_token_count : UInt
} derive(Show, Eq)

///| Result of generating embeddings
pub(all) struct EmbeddingsResult {
  embeddings : Array[Array[Double]]
  usage : EmbeddingsUsage
} derive(Show, Eq)

///| Perform inferencing using the provided model and prompt with the given optional params
pub fn infer(
  model : String,
  prompt : String,
  params : InferencingParams?
) -> Result[InferencingResult, Error_] {
  let (lowered, lowered3, lowered4, lowered5, lowered6, lowered7, lowered8) = match
    params {
    None => (0, 0, (0.0 : Float), 0, (0.0 : Float), 0, (0.0 : Float))
    Some(payload2) =>
      (
        1,
        payload2.max_tokens.reinterpret_as_int(),
        payload2.repeat_penalty.to_float(),
        payload2.repeat_penalty_last_n_token_count.reinterpret_as_int(),
        payload2.temperature.to_float(),
        payload2.top_k.reinterpret_as_int(),
        payload2.top_p.to_float(),
      )
  }
  let return_area = @ffi.malloc(20)
  wasmImportInfer(
    @ffi.str2ptr(model),
    model.iter().count(),
    @ffi.str2ptr(prompt),
    prompt.iter().count(),
    lowered,
    lowered3,
    lowered4,
    lowered5,
    lowered6,
    lowered7,
    lowered8,
    return_area,
  )
  let lifted21 = match @ffi.load8_u(return_area + 0) {
    0 => {
      let result = @ffi.ptr2str(@ffi.load32(return_area + 4))
      Result::Ok(
        InferencingResult::{
          text: result,
          usage: InferencingUsage::{
            prompt_token_count: @ffi.load32(return_area + 12).reinterpret_as_uint(),
            generated_token_count: @ffi.load32(return_area + 16).reinterpret_as_uint(),
          },
        },
      )
    }
    1 => {
      let lifted = match @ffi.load8_u(return_area + 4) {
        0 => Error_::ModelNotSupported
        1 => {
          let result17 = @ffi.ptr2str(@ffi.load32(return_area + 8))
          Error_::RuntimeError(result17)
        }
        2 => {
          let result20 = @ffi.ptr2str(@ffi.load32(return_area + 8))
          Error_::InvalidInput(result20)
        }
        _ => panic()
      }
      Result::Err(lifted)
    }
    _ => panic()
  }
  ignore(model)
  ignore(prompt)
  @ffi.free(return_area)
  return lifted21
}

///| Generate embeddings for the supplied list of text
pub fn generate_embeddings(
  model : String,
  text : Array[String]
) -> Result[EmbeddingsResult, Error_] {
  let cleanupList : Array[@ffi.Cleanup] = []
  let ignoreList : Array[@ffi.Any] = []
  let address = @ffi.malloc(text.length() * 8)
  for index = 0; index < text.length(); index = index + 1 {
    let element : String = text[index]
    let base = address + index * 8
    @ffi.store32(base + 4, element.iter().count())
    @ffi.store32(base + 0, @ffi.str2ptr(element))
    ignoreList.push(element)
  }
  let return_area = @ffi.malloc(16)
  wasmImportGenerateEmbeddings(
    @ffi.str2ptr(model),
    model.iter().count(),
    address,
    text.length(),
    return_area,
  )
  let lifted18 = match @ffi.load8_u(return_area + 0) {
    0 => {
      let array7 : Array[Array[Double]] = []
      for index8 = 0; index8 < @ffi.load32(return_area + 8); index8 = index8 + 1 {
        let base3 = @ffi.load32(return_area + 4) + index8 * 8
        let array : Array[Double] = []
        for index6 = 0; index6 < @ffi.load32(base3 + 4); index6 = index6 + 1 {
          let base5 = @ffi.load32(base3 + 0) + index6 * 4
          array.push(@ffi.loadf32(base5 + 0).to_double())
        }
        @ffi.free(@ffi.load32(base3 + 0))
        array7.push(array)
      }
      @ffi.free(@ffi.load32(return_area + 4))
      Result::Ok(
        EmbeddingsResult::{
          embeddings: array7,
          usage: EmbeddingsUsage::{
            prompt_token_count: @ffi.load32(return_area + 12).reinterpret_as_uint(),
          },
        },
      )
    }
    1 => {
      let lifted = match @ffi.load8_u(return_area + 4) {
        0 => Error_::ModelNotSupported
        1 => {
          let result = @ffi.ptr2str(@ffi.load32(return_area + 8))
          Error_::RuntimeError(result)
        }
        2 => {
          let result17 = @ffi.ptr2str(@ffi.load32(return_area + 8))
          Error_::InvalidInput(result17)
        }
        _ => panic()
      }
      Result::Err(lifted)
    }
    _ => panic()
  }
  ignore(model)
  @ffi.free(address)
  @ffi.free(return_area)
  cleanupList.each(fn(cleanup) { @ffi.free(cleanup.address) })
  ignore(ignoreList)
  return lifted18
}
